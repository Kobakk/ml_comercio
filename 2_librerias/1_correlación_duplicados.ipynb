{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfA4atG4JiSmObUouGFWCp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Concepto de Correlación"],"metadata":{"id":"37-wcLFNb9jW"}},{"cell_type":"markdown","source":["## Generación de datos sintéticos, con Numpy\n","\n","*  30 muestras\n","*   Variable horas. Aleatorio entre 0 y 10 horas  \n","*   Variable ruido. Aleatorio entre 0 y 100 distribuido uniformemente.\n","*   Variable calificaciones que dependerá de horas de estudio más ruido (3*horas de estudio + ruido)\n","¿Qué tipo de relación es ésta?\n","*   Generar matriz de correlación de Pearson\n","\n"],"metadata":{"id":"G6izFUWVcC60"}},{"cell_type":"code","source":["import numpy as np\n","muestras= 80\n","np.random.seed(30)\n","horas = np.random.randint(0,10,muestras)\n","ruido= np.random.uniform(-10,10,muestras)\n","calificaciones=3*horas\n","calificaciones_ruido=3*horas+ruido\n","print(f\"horas\\tcalficaciones\")\n","\n","data = np.column_stack([horas, calificaciones,calificaciones_ruido])\n","\n","print(data)\n","print (f\"Horas-calificaciones\")\n","matriz = np.corrcoef(horas,calificaciones)\n","print(f\"\\n{matriz}\")\n","print (f\"Horas-calificaciones_ruido\")\n","matriz_ruido = np.corrcoef(horas,calificaciones_ruido)\n","print(f\"\\n{matriz_ruido}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lToJjAlJj96J","executionInfo":{"status":"ok","timestamp":1764083836697,"user_tz":-60,"elapsed":40,"user":{"displayName":"Ana Pérez Seijas","userId":"05019112249999957847"}},"outputId":"f9b3ad25-e6e2-44df-8ba9-9ff9b0f19308"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["horas\tcalficaciones\n","[[ 5.         15.         14.86927386]\n"," [ 5.         15.          7.8722425 ]\n"," [ 4.         12.          2.61643783]\n"," [ 7.         21.         29.78311456]\n"," [ 2.          6.          1.1367543 ]\n"," [ 5.         15.          6.14269206]\n"," [ 1.          3.          0.75416449]\n"," [ 3.          9.          6.14193495]\n"," [ 9.         27.         36.72768286]\n"," [ 7.         21.         29.79181275]\n"," [ 7.         21.         22.65550447]\n"," [ 1.          3.         12.86206268]\n"," [ 1.          3.         12.79829001]\n"," [ 3.          9.         16.30112748]\n"," [ 2.          6.         -0.76563543]\n"," [ 2.          6.          3.36980481]\n"," [ 4.         12.          8.1345332 ]\n"," [ 4.         12.          8.81726486]\n"," [ 6.         18.         13.80955495]\n"," [ 0.          0.          2.75728146]\n"," [ 6.         18.          9.01308006]\n"," [ 0.          0.          8.70368621]\n"," [ 7.         21.         11.51689674]\n"," [ 4.         12.         12.63278104]\n"," [ 2.          6.         -2.56792411]\n"," [ 1.          3.          6.82988877]\n"," [ 7.         21.         13.34175024]\n"," [ 1.          3.         -3.12974985]\n"," [ 9.         27.         35.24762993]\n"," [ 8.         24.         20.21424829]\n"," [ 5.         15.         11.31285007]\n"," [ 9.         27.         33.14060223]\n"," [ 7.         21.         19.19921149]\n"," [ 9.         27.         27.23886719]\n"," [ 1.          3.         -3.96787403]\n"," [ 8.         24.         16.68700549]\n"," [ 7.         21.         11.30912102]\n"," [ 4.         12.          9.58026483]\n"," [ 6.         18.          9.93298991]\n"," [ 8.         24.         22.84373192]\n"," [ 9.         27.         21.3333663 ]\n"," [ 6.         18.         24.5380472 ]\n"," [ 0.          0.          3.59571467]\n"," [ 7.         21.         16.38859782]\n"," [ 5.         15.         24.13825556]\n"," [ 9.         27.         36.66784515]\n"," [ 3.          9.          4.93555271]\n"," [ 1.          3.          5.38972897]\n"," [ 4.         12.          5.41782447]\n"," [ 2.          6.         -1.96716843]\n"," [ 3.          9.         17.06092623]\n"," [ 9.         27.         17.98118344]\n"," [ 8.         24.         16.91545096]\n"," [ 0.          0.         -2.01451032]\n"," [ 7.         21.         22.82563056]\n"," [ 1.          3.          6.5469604 ]\n"," [ 9.         27.         36.77317082]\n"," [ 4.         12.          8.93501709]\n"," [ 2.          6.          1.72791661]\n"," [ 1.          3.          7.10343618]\n"," [ 9.         27.         25.57616433]\n"," [ 7.         21.         27.40184659]\n"," [ 6.         18.         25.38839107]\n"," [ 0.          0.          5.60695673]\n"," [ 7.         21.         15.12967283]\n"," [ 0.          0.          7.03034179]\n"," [ 3.          9.          6.36669448]\n"," [ 6.         18.         26.00512383]\n"," [ 7.         21.         16.87822147]\n"," [ 3.          9.         11.76256589]\n"," [ 3.          9.         14.78821201]\n"," [ 5.         15.          9.58259804]\n"," [ 7.         21.         25.45142867]\n"," [ 4.         12.          7.51582144]\n"," [ 7.         21.         12.00861505]\n"," [ 3.          9.         -0.37866551]\n"," [ 6.         18.         21.93962969]\n"," [ 7.         21.         26.44655325]\n"," [ 2.          6.         13.19510699]\n"," [ 3.          9.          3.89474511]]\n","Horas-calificaciones\n","\n","[[1. 1.]\n"," [1. 1.]]\n","Horas-calificaciones_ruido\n","\n","[[1.         0.79321845]\n"," [0.79321845 1.        ]]\n"]}]},{"cell_type":"markdown","source":["Dado un dataset con columnas numericas, calcular el grado de correlación entre las columnas y eliminar las columnas que tengan un grado de correlación superior a 0.90\n","Crear un dataset de 3x6.\n","\n","\n"],"metadata":{"id":"CSfv-Sbhvj-N"}},{"cell_type":"code","source":["import numpy as np\n","\n","np.random.seed(0)\n","\n","rows = 25\n","\n","col1 = np.random.randn(rows)\n","col2 = col1 * 0.8 + np.random.randn(rows) * 0.2      # correlación alta\n","col3 = np.random.randn(rows)                         # poca correlación\n","col4 = col1 * 0.95 + np.random.randn(rows) * 0.05    # correlación muy alta\n","col5 = 0.5 * col1 + 0.5 * col3                        # correlación mixta\n","\n","data = np.column_stack([col1, col2, col3, col4, col5])\n","\n","print(\"Dataset sintético (primeras filas):\")\n","print(data[:5])\n","\n","matriz =np.corrcoef(data,rowvar=False)\n","mask_correlacion_superior_0_90=np.abs(matriz)>0.90\n","print(mask_correlacion_superior_0_90)\n","\n","np.fill_diagonal(mask_correlacion_superior_0_90,False) # ignoramos la diagonal\n","\n","# 3. Detectar columnas a eliminar\n","n_cols = mask_correlacion_superior_0_90.shape[0]\n","cols_a_eliminar = np.zeros(n_cols, dtype=bool)\n","\n","for i in range(n_cols):\n","    if not cols_a_eliminar[i]:\n","        # Si la columna i tiene correlación alta con otra columna j\n","        correlated = mask_correlacion_superior_0_90[i] & (~cols_a_eliminar)  # ignorar ya eliminadas\n","        # Eliminamos todas las columnas correlacionadas j > i\n","        cols_a_eliminar = cols_a_eliminar | correlated\n","\n","# 4. Crear dataset reducido\n","data_reducido = data[:, ~cols_a_eliminar]\n","\n","print(\"Columnas a eliminar (boolean mask):\", cols_a_eliminar)\n","print(\"\\nDataset reducido (primeras filas):\")\n","print(data_reducido[:5])"],"metadata":{"id":"r4JzQwt-wDbl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764084004097,"user_tz":-60,"elapsed":27,"user":{"displayName":"Ana Pérez Seijas","userId":"05019112249999957847"}},"outputId":"d2919cf3-bbbf-4740-d096-f2d2c70e863c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset sintético (primeras filas):\n","[[ 1.76405235  1.12036874 -0.89546656  1.64160922  0.43429289]\n"," [ 0.40015721  0.32927747  0.3869025   0.33660949  0.39352985]\n"," [ 0.97873798  0.74555362 -0.51080514  0.9008586   0.23396642]\n"," [ 2.2408932   2.0992704  -1.18063218  2.11327091  0.53013051]\n"," [ 1.86755799  1.78791815 -0.02818223  1.77698836  0.91968788]]\n","[[ True  True False  True False]\n"," [ True  True False  True False]\n"," [False False  True False False]\n"," [ True  True False  True False]\n"," [False False False False  True]]\n","Columnas a eliminar (boolean mask): [False  True False  True False]\n","\n","Dataset reducido (primeras filas):\n","[[ 1.76405235 -0.89546656  0.43429289]\n"," [ 0.40015721  0.3869025   0.39352985]\n"," [ 0.97873798 -0.51080514  0.23396642]\n"," [ 2.2408932  -1.18063218  0.53013051]\n"," [ 1.86755799 -0.02818223  0.91968788]]\n"]}]},{"cell_type":"markdown","source":["Dado un dataset, dividir en 20% para entrenamiento y un 80 para predicción"],"metadata":{"id":"3HInisS1wBzV"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Dataset de ejemplo\n","np.random.seed(0)\n","data = np.random.randn(25, 5)\n","\n","# Proporciones\n","train_frac = 0.2\n","train_size = int(data.shape[0] * train_frac)\n","\n","# Seleccionar índices de entrenamiento aleatoriamente, sin repetición\n","train_idx = np.random.choice(data.shape[0], size=train_size, replace=False)\n","\n","# Crear máscara booleana para identificar filas de entrenamiento\n","mask = np.zeros(data.shape[0], dtype=bool)\n","mask[train_idx] = True\n","\n","# Índices de test son los que no están en entrenamiento\n","test_idx = np.where(~mask)[0]\n","\n","# Crear splits\n","train_data = data[train_idx]\n","test_data = data[test_idx]\n","\n","# Mostrar resultados\n","print(\"Indices de entrenamiento:\", train_idx)\n","print(\"Indices de predicción:\", test_idx)\n","\n","print(\"\\nDatos de entrenamiento:\\n\", train_data)\n","print(\"\\nDatos de predicción:\\n\", test_data)\n","\n","\n"],"metadata":{"id":"hJbIZiYOwP0E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764083836752,"user_tz":-60,"elapsed":20,"user":{"displayName":"Ana Pérez Seijas","userId":"05019112249999957847"}},"outputId":"284503ce-778a-4e13-b3bf-47ffa3c8ca2e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Indices de entrenamiento: [ 6  8  4 20 19]\n","Indices de predicción: [ 0  1  2  3  5  7  9 10 11 12 13 14 15 16 17 18 21 22 23 24]\n","\n","Datos de entrenamiento:\n"," [[ 0.15494743  0.37816252 -0.88778575 -1.98079647 -0.34791215]\n"," [-1.04855297 -1.42001794 -1.70627019  1.9507754  -0.50965218]\n"," [-2.55298982  0.6536186   0.8644362  -0.74216502  2.26975462]\n"," [ 1.8831507  -1.34775906 -1.270485    0.96939671 -1.17312341]\n"," [ 0.70657317  0.01050002  1.78587049  0.12691209  0.40198936]]\n","\n","Datos de predicción:\n"," [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799]\n"," [-0.97727788  0.95008842 -0.15135721 -0.10321885  0.4105985 ]\n"," [ 0.14404357  1.45427351  0.76103773  0.12167502  0.44386323]\n"," [ 0.33367433  1.49407907 -0.20515826  0.3130677  -0.85409574]\n"," [-1.45436567  0.04575852 -0.18718385  1.53277921  1.46935877]\n"," [ 0.15634897  1.23029068  1.20237985 -0.38732682 -0.30230275]\n"," [-0.4380743  -1.25279536  0.77749036 -1.61389785 -0.21274028]\n"," [-0.89546656  0.3869025  -0.51080514 -1.18063218 -0.02818223]\n"," [ 0.42833187  0.06651722  0.3024719  -0.63432209 -0.36274117]\n"," [-0.67246045 -0.35955316 -0.81314628 -1.7262826   0.17742614]\n"," [-0.40178094 -1.63019835  0.46278226 -0.90729836  0.0519454 ]\n"," [ 0.72909056  0.12898291  1.13940068 -1.23482582  0.40234164]\n"," [-0.68481009 -0.87079715 -0.57884966 -0.31155253  0.05616534]\n"," [-1.16514984  0.90082649  0.46566244 -1.53624369  1.48825219]\n"," [ 1.89588918  1.17877957 -0.17992484 -1.07075262  1.05445173]\n"," [-0.40317695  1.22244507  0.20827498  0.97663904  0.3563664 ]\n"," [ 1.94362119 -0.41361898 -0.74745481  1.92294203  1.48051479]\n"," [ 1.86755896  0.90604466 -0.86122569  1.91006495 -0.26800337]\n"," [ 0.8024564   0.94725197 -0.15501009  0.61407937  0.92220667]\n"," [ 0.37642553 -1.09940079  0.29823817  1.3263859  -0.69456786]]\n"]}]},{"cell_type":"markdown","source":["Averiguar si un dataset tiene filas repetidas y si es así eliminar dejando únicamente una.\n"],"metadata":{"id":"HXqtFb0rwQd8"}},{"cell_type":"code","source":["import numpy as np\n","# 1. Crear un dataset con filas duplicadas\n","data = np.array([\n","    [1, 2, 3],\n","    [4, 5, 6],\n","    [1, 2, 3],   # duplicada\n","    [7, 8, 9],\n","    [4, 5, 6],   # duplicada\n","    [10, 11, 12]\n","])\n","\n","print(\"Dataset original:\")\n","print(data)\n","\n","# 2. Detectar y eliminar filas duplicadas\n","# np.unique con axis=0 devuelve solo filas únicas\n","unique_rows = np.unique(data, axis=0)\n","\n","print(\"\\nDataset sin filas duplicadas:\")\n","print(unique_rows)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2ilC7xL41Sr","executionInfo":{"status":"ok","timestamp":1764083836781,"user_tz":-60,"elapsed":26,"user":{"displayName":"Ana Pérez Seijas","userId":"05019112249999957847"}},"outputId":"3d98cc8a-548e-46ea-a19a-8476ca76f16a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset original:\n","[[ 1  2  3]\n"," [ 4  5  6]\n"," [ 1  2  3]\n"," [ 7  8  9]\n"," [ 4  5  6]\n"," [10 11 12]]\n","\n","Dataset sin filas duplicadas:\n","[[ 1  2  3]\n"," [ 4  5  6]\n"," [ 7  8  9]\n"," [10 11 12]]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","data = np.array([\n","    [1, 2, 3],\n","    [4, 5, 6],\n","    [1, 2, 3],   # duplicada\n","    [7, 8, 9],\n","    [4, 5, 6],   # duplicada\n","    [1, 2, 3]    # duplicada\n","])\n","\n","# Obtener información de unicidad\n","unique_rows, first_idx, inv, counts = np.unique(\n","    data, axis=0,\n","    return_index=True,\n","    return_inverse=True,\n","    return_counts=True\n",")\n","\n","\n","# Lista donde guardaremos los índices duplicados\n","duplicate_indices = []\n","\n","# Para cada fila única, detectar todas sus apariciones\n","for i, c in enumerate(counts):\n","    if c > 1:  # si aparece más de una vez\n","        # posiciones donde aparece esta fila\n","        all_occ = np.where(inv == i)[0]\n","        # agregar todas excepto la primera\n","        duplicate_indices.extend(all_occ[1:])\n","\n","print(\"Índices de filas duplicadas:\", duplicate_indices)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_TgqldY5JHS","executionInfo":{"status":"ok","timestamp":1764083836784,"user_tz":-60,"elapsed":17,"user":{"displayName":"Ana Pérez Seijas","userId":"05019112249999957847"}},"outputId":"e9eea4a0-e997-446b-9e1e-a98adb8df991"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Índices de filas duplicadas: [np.int64(2), np.int64(5), np.int64(4)]\n"]}]}]}